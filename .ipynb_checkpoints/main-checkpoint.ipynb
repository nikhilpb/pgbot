{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "de07ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "import numpy as np\n",
    "import requests\n",
    "import spacy\n",
    "import random\n",
    "import re\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fa583f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched the articles page.\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = 'http://www.paulgraham.com/'\n",
    "ARTICLES_URL = BASE_URL + 'articles.html'\n",
    "BASE_URL\n",
    "response = requests.get(ARTICLES_URL)\n",
    "if response.status_code == 200:\n",
    "    print('Fetched the articles page.')\n",
    "else:\n",
    "    print(f'Request failed with code {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8ccca67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles fetched: 218\n"
     ]
    }
   ],
   "source": [
    "pgsoup = BS(response.text, 'lxml')\n",
    "print(f\"Total articles fetched: {len(pgsoup.select('a')[1:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "999f1754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 10 out of 219 requests\n",
      "Finished 20 out of 219 requests\n",
      "Finished 30 out of 219 requests\n",
      "Finished 40 out of 219 requests\n",
      "Finished 50 out of 219 requests\n",
      "Finished 60 out of 219 requests\n",
      "Finished 70 out of 219 requests\n",
      "Finished 80 out of 219 requests\n",
      "Finished 90 out of 219 requests\n",
      "Finished 100 out of 219 requests\n",
      "Finished 110 out of 219 requests\n",
      "Finished 120 out of 219 requests\n",
      "Finished 130 out of 219 requests\n",
      "Finished 140 out of 219 requests\n",
      "Finished 150 out of 219 requests\n",
      "Finished 160 out of 219 requests\n",
      "Finished 170 out of 219 requests\n",
      "Finished 180 out of 219 requests\n",
      "Finished 190 out of 219 requests\n",
      "Finished 200 out of 219 requests\n",
      "Finished 210 out of 219 requests\n",
      "total_success: 217, total_failure: 2\n"
     ]
    }
   ],
   "source": [
    "all_links = [BASE_URL + link.get('href') for link in pgsoup.find_all('a')]\n",
    "success = 0\n",
    "failure = 0\n",
    "finished_requests = 0\n",
    "all_pages = []\n",
    "total_requests = len(all_links)\n",
    "for link in all_links:\n",
    "    finished_requests = finished_requests + 1\n",
    "    rs = requests.get(link)\n",
    "    if finished_requests % 10 == 0:\n",
    "        print(f'Finished {finished_requests} out of {total_requests} requests')\n",
    "    if rs.status_code == 200:\n",
    "        success = success + 1\n",
    "        all_pages.append(rs.text)\n",
    "    else: \n",
    "        failure = failure + 1\n",
    "print(f'total_success: {success}, total_failure: {failure}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e3e06e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately 4.44 MB of data fetched\n"
     ]
    }
   ],
   "source": [
    "print(f'Approximately {float(sum([len(t) for t in all_pages])) / (1024 * 1024):.2f} MB of data fetched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a3c81d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "essays = [BS(p).find('table') for p in all_pages]\n",
    "essays = [e.text.replace(\"\\'\", \"\").replace(\"\\n\", \" \") for e in essays if (e is not None and len(e.text) >= 500)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7a0ec34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately 2.81 MB of essays\n"
     ]
    }
   ],
   "source": [
    "print(f'Approximately {float(sum([len(e) for e in essays])) / (1024 * 1024):.2f} MB of essays')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "17acba46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 12.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from en-core-web-sm==3.4.1) (3.4.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: jinja2 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: setuptools in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (65.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kritikakaul/opt/anaconda3/envs/py310/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download('en_core_web_sm')\n",
    "tk = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e4ea09a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [tk(e)[1:] for e in essays] \n",
    "for t in tokens:\n",
    "    t[0] = t[0][4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "33b1ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(essays):\n",
    "    for text in essays:\n",
    "        yield tk(text)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(essays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "557d7fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 10\n",
    "embeds = nn.Embedding(len(vocab), EMBED_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ef40e7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2555, -0.2704,  0.7193,  1.0877, -1.1312,  0.0713, -0.3876, -0.3571,\n",
       "          1.2550, -0.1371]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_tensor = torch.tensor([vocab['hello']], dtype=torch.long)\n",
    "embeds(lookup_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "57b46180",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 3\n",
    "all_tokens = []\n",
    "for ts in tokens:\n",
    "    for t in ts:\n",
    "        all_tokens.append(t)\n",
    "ngrams = [\n",
    "    (\n",
    "        [all_tokens[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        all_tokens[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(all_tokens))\n",
    "]\n",
    "\n",
    "n1 = int(0.8 * len(ngrams))\n",
    "n2 = int(0.8 * len(ngrams))\n",
    "ngrams_train = ngrams[0:n1]\n",
    "ngrams_valid = ngrams[n1:n2]\n",
    "ngrams_tes = ngrams[n2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "40f93cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615966"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "c1b3bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBED_SIZE, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "37c5077e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2855, -0.2556, -0.3491,  1.0564,  0.4563,  1.3726, -1.8844, -0.7771,\n",
       "         -0.2747, -0.2610],\n",
       "        [ 1.3154, -0.7622,  0.8609,  0.9889, -0.3409,  1.0949,  0.3211,  0.0279,\n",
       "          1.4135,  0.8031]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds(torch.tensor(vocab(['I', 'am'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab2e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 steps done.\n",
      "200 steps done.\n",
      "300 steps done.\n",
      "400 steps done.\n",
      "500 steps done.\n",
      "600 steps done.\n",
      "700 steps done.\n",
      "800 steps done.\n",
      "900 steps done.\n",
      "1000 steps done.\n",
      "1100 steps done.\n",
      "1200 steps done.\n",
      "1300 steps done.\n",
      "1400 steps done.\n",
      "1500 steps done.\n",
      "1600 steps done.\n",
      "1700 steps done.\n",
      "1800 steps done.\n",
      "1900 steps done.\n",
      "2000 steps done.\n",
      "10372.334538988769\n",
      "100 steps done.\n",
      "200 steps done.\n",
      "300 steps done.\n",
      "400 steps done.\n",
      "500 steps done.\n",
      "600 steps done.\n",
      "700 steps done.\n",
      "800 steps done.\n",
      "900 steps done.\n",
      "1000 steps done.\n",
      "1100 steps done.\n",
      "1200 steps done.\n",
      "1300 steps done.\n",
      "1400 steps done.\n",
      "1500 steps done.\n",
      "1600 steps done.\n",
      "1700 steps done.\n",
      "1800 steps done.\n",
      "1900 steps done.\n",
      "2000 steps done.\n",
      "10177.664019532502\n",
      "100 steps done.\n",
      "200 steps done.\n",
      "300 steps done.\n",
      "400 steps done.\n",
      "500 steps done.\n",
      "600 steps done.\n",
      "700 steps done.\n",
      "800 steps done.\n",
      "900 steps done.\n",
      "1000 steps done.\n",
      "1100 steps done.\n",
      "1200 steps done.\n",
      "1300 steps done.\n",
      "1400 steps done.\n",
      "1500 steps done.\n",
      "1600 steps done.\n",
      "1700 steps done.\n",
      "1800 steps done.\n",
      "1900 steps done.\n",
      "2000 steps done.\n",
      "9987.494414538145\n",
      "100 steps done.\n",
      "200 steps done.\n",
      "300 steps done.\n",
      "400 steps done.\n",
      "500 steps done.\n",
      "600 steps done.\n",
      "700 steps done.\n",
      "800 steps done.\n",
      "900 steps done.\n",
      "1000 steps done.\n",
      "1100 steps done.\n",
      "1200 steps done.\n",
      "1300 steps done.\n",
      "1400 steps done.\n",
      "1500 steps done.\n",
      "1600 steps done.\n",
      "1700 steps done.\n",
      "1800 steps done.\n",
      "1900 steps done.\n",
      "2000 steps done.\n",
      "9800.924741365016\n",
      "100 steps done.\n",
      "200 steps done.\n",
      "300 steps done.\n",
      "400 steps done.\n",
      "500 steps done.\n",
      "600 steps done.\n",
      "700 steps done.\n",
      "800 steps done.\n",
      "900 steps done.\n",
      "1000 steps done.\n",
      "1100 steps done.\n",
      "1200 steps done.\n",
      "1300 steps done.\n",
      "1400 steps done.\n",
      "1500 steps done.\n",
      "1600 steps done.\n",
      "1700 steps done.\n",
      "1800 steps done.\n",
      "1900 steps done.\n",
      "2000 steps done.\n",
      "9617.163951456547\n",
      "100 steps done.\n",
      "200 steps done.\n",
      "300 steps done.\n",
      "400 steps done.\n",
      "500 steps done.\n",
      "600 steps done.\n",
      "700 steps done.\n",
      "800 steps done.\n",
      "900 steps done.\n",
      "1000 steps done.\n",
      "1100 steps done.\n",
      "1200 steps done.\n",
      "1300 steps done.\n",
      "1400 steps done.\n",
      "1500 steps done.\n",
      "1600 steps done.\n",
      "1700 steps done.\n",
      "1800 steps done.\n",
      "1900 steps done.\n",
      "2000 steps done.\n",
      "9435.63080265373\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    for context, target in ngrams_train[0:2000]:\n",
    "        if target not in vocab:\n",
    "            continue\n",
    "        steps = steps + 1\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([vocab[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([vocab[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        if steps % 100 == 0:\n",
    "            print(f'{steps} steps done.')\n",
    "    print(total_loss)\n",
    "    losses.append(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "cfcf3280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5463, -0.0729, -1.7869,  0.2753, -0.0032,  0.6494, -0.2053, -0.3618,\n",
       "        -0.2261,  1.1085], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings(torch.tensor(vocab['sad'], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915eec14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
